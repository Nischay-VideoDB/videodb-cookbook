{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∏üó£Ô∏è Multimodal Quickstart \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/quickstart/multimodal_quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "Narration is the heartbeat of trailers, injecting excitement and intrigue into every frame ‚ñ∂Ô∏è With [VideoDB](https://videodb.io), [OpenAI](https://openai.com), and [ElevenLabs](https://elevenlabs.io) at your fingertips, adding narration to trailers becomes a creative process. This tutorial will guide you through the simple process of seamlessly integrating narration into trailers using these powerful tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶  Installing packages "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install openai\n",
    "%pip install videodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë API keys\n",
    "Before proceeding, ensure access to [VideoDB](https://videodb.io), [OpenAI](https://openai.com). If not, sign up for API access on the respective platforms.\n",
    "\n",
    "> Get your API key from [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required** ) üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Multimodal Search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB\n",
    "\n",
    "Gear up by establishing a connection to VideoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# Connect to VideoDB using your API key\n",
    "conn = connect()\n",
    "coll = conn.get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Step 2: Upload the Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = coll.upload(url=\"https://www.youtube.com/watch?v=libKVRa01L8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏üó£Ô∏è Step 3: Index the Video on different Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Indexing Spoken Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index spoken content\n",
    "\n",
    "video.index_spoken_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∏Ô∏è **Indexing Visual Content**\n",
    "\n",
    "#TODO: Link VideoDB Scene Index Quickstart and configuration links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "# Index scene content\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 2, \"select_frames\": ['first', 'last']},\n",
    "    prompt=\"Describe the scene in detail\"\n",
    ")\n",
    "video.get_scene_index(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Step 4: Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken Query: Find the segment where the narrator talks about the terrestrial planets\n",
      "Visual Query: Show images of Mercury, Venus, Earth, and Mars\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "transformation_prompt = \"\"\"\n",
    "Divide the following query into two distinct parts: one for spoken content and one for visual content. The spoken content should refer to any narration, dialogue, or verbal explanations and The visual content should refer to any images, videos, or graphical representations. Format the response strictly as:\\nSpoken: <spoken_query>\\nVisual: <visual_query>\\n\\nQuery: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "def divide_query(query):\n",
    "    # Use the OpenAI client to create a chat completion with a structured prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transformation_prompt.format(query=query)\n",
    "            }\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message.content\n",
    "    divided_query = message.strip().split('\\n')\n",
    "    spoken_query = divided_query[0].replace('Spoken:', '').strip()\n",
    "    visual_query = divided_query[1].replace('Visual:', '').strip()\n",
    "\n",
    "    return spoken_query, visual_query\n",
    "\n",
    "\n",
    "# Test the query\n",
    "query = \"Find the segment where the narrator talks about the terrestrial planets and show images of Mercury, Venus, Earth, and Mars\"\n",
    "#Example query 2 :- Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy.\n",
    "\n",
    "spoken_query, visual_query = divide_query(query)\n",
    "print(f\"Spoken Query: {spoken_query}\")\n",
    "print(f\"Visual Query: {visual_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 5:  Performing Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Search from Spoken Index**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO: if anywhere we have explained about thresholds link that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/7824e096-9a04-4bf0-8f98-1aac9723edc7.m3u8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import SearchType, IndexType\n",
    "\n",
    "# Perform the search using the spoken query\n",
    "spoken_results = video.search(query=spoken_query, index_type=IndexType.spoken_word, search_type=SearchType.semantic)\n",
    "\n",
    "# View the results\n",
    "spoken_results.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∏Ô∏è **Searching from Scene Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/fec5712e-1e5b-419a-b86a-d1c8104d8e4b.m3u8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the search using the visual query\n",
    "scene_results = video.search(query=visual_query, index_type=IndexType.scene, search_type=SearchType.semantic)\n",
    "\n",
    "# View the results\n",
    "scene_results.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÄ Step 6: Combining Spoken & Scene Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each search result provides a list of timestamps that are relevant to the query in the relevant modality.\n",
    "\n",
    "There are two ways to combine these search results:\n",
    "\n",
    "- **Union**: This method takes all the timestamps from every search result, creating a comprehensive list that includes every relevant time, even if some timestamps appear in only one result.\n",
    "\n",
    "- **Intersection**: This method only includes timestamps that appear in all the search results, resulting in a smaller list with times that are universally relevant across all results.\n",
    "\n",
    "Depending on which method you prefer, you can pass the appropriate argument to the `combine_results()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken results:  [[37.026, 79.33]]\n",
      "Scene results:  [[44.044, 46.046], [54.054, 56.056]]\n",
      "Combined results:  [[44.044, 46.046], [54.054, 56.056]]\n"
     ]
    }
   ],
   "source": [
    "#Define a Function to Find Intersections\n",
    "def process_shots(l1, l2, operation):\n",
    "    def merge_intervals(intervals):\n",
    "        if not intervals:\n",
    "            return []\n",
    "        intervals.sort(key=lambda x: x[0])\n",
    "        merged = [intervals[0]]\n",
    "        for interval in intervals[1:]:\n",
    "            if interval[0] <= merged[-1][1]:\n",
    "                merged[-1][1] = max(merged[-1][1], interval[1])\n",
    "            else:\n",
    "                merged.append(interval)\n",
    "        return merged\n",
    "\n",
    "    def intersection(intervals1, intervals2):\n",
    "        i, j = 0, 0\n",
    "        result = []\n",
    "        while i < len(intervals1) and j < len(intervals2):\n",
    "            low = max(intervals1[i][0], intervals2[j][0])\n",
    "            high = min(intervals1[i][1], intervals2[j][1])\n",
    "            if low < high:\n",
    "                result.append([low, high])\n",
    "            if intervals1[i][1] < intervals2[j][1]:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return result\n",
    "\n",
    "    if operation.lower() == 'intersection':\n",
    "        return intersection(merge_intervals(l1), merge_intervals(l2))\n",
    "    elif operation.lower() == 'union':\n",
    "        return merge_intervals(l1 + l2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation. Please choose 'intersection' or 'union'.\")\n",
    "\n",
    "def combine_results(spoken_results, scene_results, operation):\n",
    "    spoken_timestamps = [[shot.start, shot.end] for shot in spoken_results.get_shots()]\n",
    "    scene_timestamps = [[shot.start, shot.end] for shot in scene_results.get_shots()]\n",
    "    result = process_shots(spoken_timestamps, scene_timestamps, operation)\n",
    "    print(\"Spoken results: \", spoken_timestamps)\n",
    "    print(\"Scene results: \", scene_timestamps)\n",
    "    print(\"Combined results: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Combine results \n",
    "results = combine_results(spoken_results, scene_results, \"intersection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™Ñ Step7 : View Combined Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/6f920601-41ca-4977-9767-8e5ecebdc3b9.m3u8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "stream_link = video.generate_stream(results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "\n",
    "- Experiment on Scene Extraction pipeline\n",
    "- Experiment with Scene Indexing Prompt\n",
    "- Experiment on Query Transformation\n",
    "- Explore more way to combine search results\n",
    "\n",
    "\n",
    "Links to other blogs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
