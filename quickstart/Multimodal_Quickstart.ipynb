{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì∏üó£Ô∏è Multimodal Quickstart \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/quickstart/Multimodal_Quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "This tutorial demonstrates how to perform multimodal search on videos using VideoDB and OpenAI. We'll accomplish this by searching both spoken and scene indexes and finding their intersection points.\n",
    "\n",
    "In this tutorial, we'll:\n",
    "\n",
    "1. Upload and index a sample educational video about the solar system\n",
    "2. Use OpenAI to divide a query into spoken and visual components\n",
    "3. Perform searches in both spoken word and scene indexes\n",
    "4. Find intersecting segments between the two search results\n",
    "5. Visualize the final combined results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶  Installing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openai\n",
    "%pip install videodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë API keys\n",
    "Before proceeding, ensure access to [VideoDB](https://videodb.io), [OpenAI](https://openai.com). If not, sign up for API access on the respective platforms.\n",
    "\n",
    "> Get your API key from [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required** ) üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Multimodal Search\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB\n",
    "\n",
    "Gear up by establishing a connection to VideoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# Connect to VideoDB using your API key\n",
    "conn = connect()\n",
    "coll = conn.get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Step 2: Upload the Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = coll.upload(url=\"https://www.youtube.com/watch?v=libKVRa01L8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏üó£Ô∏è Step 3: Index the Video on different Modalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üó£Ô∏è Indexing Spoken Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index spoken content\n",
    "\n",
    "video.index_spoken_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üì∏Ô∏è Indexing Visual Content\n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) guide provides a step-by-step introduction to Scene Index. It's ideal for getting started quickly and understanding the primary functions.\n",
    "\n",
    "- [Scene Extraction Options Guide](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb) delves deeper into the various options available for scene extraction within Scene Index. It covers advanced settings, customization features, and tips for optimizing scene extraction based on different needs and preferences.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "# Index scene content\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 2, \"select_frames\": [\"first\", \"last\"]},\n",
    "    prompt=\"Describe the scene in detail\",\n",
    ")\n",
    "video.get_scene_index(index_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© Step 4: Query Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken Query: Find the segment where the narrator talks about the terrestrial planets\n",
      "Visual Query: Show images of Mercury, Venus, Earth, and Mars\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "transformation_prompt = \"\"\"\n",
    "Divide the following query into two distinct parts: one for spoken content and one for visual content. The spoken content should refer to any narration, dialogue, or verbal explanations and The visual content should refer to any images, videos, or graphical representations. Format the response strictly as:\\nSpoken: <spoken_query>\\nVisual: <visual_query>\\n\\nQuery: {query}\n",
    "\"\"\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "def divide_query(query):\n",
    "    # Use the OpenAI client to create a chat completion with a structured prompt\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": transformation_prompt.format(query=query)}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message.content\n",
    "    divided_query = message.strip().split(\"\\n\")\n",
    "    spoken_query = divided_query[0].replace(\"Spoken:\", \"\").strip()\n",
    "    visual_query = divided_query[1].replace(\"Visual:\", \"\").strip()\n",
    "\n",
    "    return spoken_query, visual_query\n",
    "\n",
    "\n",
    "# Test the query\n",
    "query = \"Find the segment where the narrator talks about the terrestrial planets and show images of Mercury, Venus, Earth, and Mars\"\n",
    "# Example query 2 :- Show me where the narrator discusses the formation of the solar system and visualize the milky way galaxy.\n",
    "\n",
    "spoken_query, visual_query = divide_query(query)\n",
    "print(f\"Spoken Query: {spoken_query}\")\n",
    "print(f\"Visual Query: {visual_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîç Step 5:  Performing Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üó£Ô∏è **Search from Spoken Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/7824e096-9a04-4bf0-8f98-1aac9723edc7.m3u8'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import SearchType, IndexType\n",
    "\n",
    "# Perform the search using the spoken query\n",
    "spoken_results = video.search(\n",
    "    query=spoken_query,\n",
    "    index_type=IndexType.spoken_word,\n",
    "    search_type=SearchType.semantic,\n",
    ")\n",
    "\n",
    "# View the results\n",
    "spoken_results.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üì∏Ô∏è **Searching from Scene Index**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/fec5712e-1e5b-419a-b86a-d1c8104d8e4b.m3u8'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the search using the visual query\n",
    "scene_results = video.search(\n",
    "    query=visual_query, \n",
    "    index_type=IndexType.scene,\n",
    "    search_type=SearchType.semantic\n",
    ")\n",
    "\n",
    "# View the results\n",
    "scene_results.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîÄ Step 6: Combining Spoken & Scene Search results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each search result provides a list of timestamps that are relevant to the query in the relevant modality.\n",
    "\n",
    "There are two ways to combine these search results:\n",
    "\n",
    "- **Union**: This method takes all the timestamps from every search result, creating a comprehensive list that includes every relevant time, even if some timestamps appear in only one result.\n",
    "\n",
    "![](https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_quickstart_union.png)\n",
    "\n",
    "\n",
    "- **Intersection**: This method only includes timestamps that appear in all the search results, resulting in a smaller list with times that are universally relevant across all results.\n",
    "\n",
    "![](https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/multimodal_quickstart_intersection.png)\n",
    "\n",
    "\n",
    "Depending on which method you prefer, you can pass the appropriate argument to the `combine_results()` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spoken results:  [[37.026, 79.33]]\n",
      "Scene results:  [[44.044, 46.046], [54.054, 56.056]]\n",
      "Combined results:  [[44.044, 46.046], [54.054, 56.056]]\n"
     ]
    }
   ],
   "source": [
    "# Define a Function to Find Intersections\n",
    "def process_shots(l1, l2, operation):\n",
    "    def merge_intervals(intervals):\n",
    "        if not intervals:\n",
    "            return []\n",
    "        intervals.sort(key=lambda x: x[0])\n",
    "        merged = [intervals[0]]\n",
    "        for interval in intervals[1:]:\n",
    "            if interval[0] <= merged[-1][1]:\n",
    "                merged[-1][1] = max(merged[-1][1], interval[1])\n",
    "            else:\n",
    "                merged.append(interval)\n",
    "        return merged\n",
    "\n",
    "    def intersection(intervals1, intervals2):\n",
    "        i, j = 0, 0\n",
    "        result = []\n",
    "        while i < len(intervals1) and j < len(intervals2):\n",
    "            low = max(intervals1[i][0], intervals2[j][0])\n",
    "            high = min(intervals1[i][1], intervals2[j][1])\n",
    "            if low < high:\n",
    "                result.append([low, high])\n",
    "            if intervals1[i][1] < intervals2[j][1]:\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return result\n",
    "\n",
    "    if operation.lower() == \"intersection\":\n",
    "        return intersection(merge_intervals(l1), merge_intervals(l2))\n",
    "    elif operation.lower() == \"union\":\n",
    "        return merge_intervals(l1 + l2)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid operation. Please choose 'intersection' or 'union'.\")\n",
    "\n",
    "\n",
    "def combine_results(spoken_results, scene_results, operation):\n",
    "    spoken_timestamps = [[shot.start, shot.end] for shot in spoken_results.get_shots()]\n",
    "    scene_timestamps = [[shot.start, shot.end] for shot in scene_results.get_shots()]\n",
    "    result = process_shots(spoken_timestamps, scene_timestamps, operation)\n",
    "    print(\"Spoken results: \", spoken_timestamps)\n",
    "    print(\"Scene results: \", scene_timestamps)\n",
    "    print(\"Combined results: \", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "# Combine results\n",
    "results = combine_results(spoken_results, scene_results, \"intersection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü™Ñ Step7 : View Combined Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://console.videodb.io/player?url=https://dseetlpshk2tb.cloudfront.net/v3/published/manifests/6f920601-41ca-4977-9767-8e5ecebdc3b9.m3u8'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from videodb import play_stream\n",
    "\n",
    "stream_link = video.generate_stream(results)\n",
    "play_stream(stream_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide, we've demonstrated how to perform multimodal search on educational videos using VideoDB and OpenAI. \n",
    "\n",
    "To learn more about Scene Index, explore the following guides:\n",
    "\n",
    "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) \n",
    "- [Scene Extraction Options](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb)\n",
    "- [Advanced Visual Search](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/advanced_visual_search.ipynb)\n",
    "- [Custom Annotation Pipelines](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/custom_annotations.ipynb)\n",
    "\n",
    "\n",
    "If you have any questions or feedback. Feel free to reach out to us üôåüèº\n",
    "\n",
    "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
    "* [GitHub](https://github.com/video-db)\n",
    "* [VideoDB](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fvideodb.io)\n",
    "* [Email](ashu@videodb.io)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
