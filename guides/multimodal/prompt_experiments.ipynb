{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úçÔ∏è Mastering Video Scene Indexing: A Deep Dive into Prompt Engineering \n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/guides/multimodal/prompt_experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "As developers working on video processing, we often face challenges in accurately indexing and describing complex scenes. This blog post explores how strategic prompt engineering can significantly enhance our ability to extract detailed information from video frames, opening up new possibilities for advanced video search and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal of the Experiment:\n",
    "\n",
    "---\n",
    "\n",
    "Our primary objective was to demonstrate how refined prompts can significantly improve search results and information extraction from video content. We aimed to create a system capable of accurately identifying objects, actions, and even emotions in various video scenes. For this particular experiment, we used video footage from a [dog show](https://www.youtube.com/watch?v=_T3n-2zOrZQ), featuring various breeds walking down a runway with their handlers, surrounded by spectators and photographers. Our goal was to create prompts that could answer detailed queries like `\"Show me the happiest moments featuring a Golden Retriever\"`  with high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì¶  Installing packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install videodb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîë API keys\n",
    "Before proceeding, ensure access to [VideoDB](https://videodb.io). If not, sign up for API access on the respective platforms.\n",
    "\n",
    "> Get your API key from [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required** ) üéâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guide Walkthrough\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìã Step 1: Connect to VideoDB\n",
    "\n",
    "Gear up by establishing a connection to VideoDB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import connect\n",
    "\n",
    "# Connect to VideoDB using your API key\n",
    "conn = connect()\n",
    "coll = conn.get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üé¨ Step 2: Upload the Video "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = coll.upload(url=\"https://www.youtube.com/watch?v=_T3n-2zOrZQ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üì∏Ô∏è Step 3: Extracting Scenes without needing to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "# Example: Time-based extraction every 15 seconds\n",
    "scene_collection = video.extract_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 15, \"select_frames\": [\"first\", \"middle\", \"last\"]},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Note`: Image upload might take time (5s-60s). Re-fetch the scene collection if `Frame.url` is None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_collection_time = video.get_scene_collection(scene_collection.id)\n",
    "print(scene_collection_time.scenes[0].frames[0].url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úçÔ∏è Step 4: Experimenting with Prompts\n",
    "\n",
    "Frame-Level vs Scene-Level Prompting:\n",
    "\n",
    "In our experiment, we explored both frame-level and scene-level prompting:\n",
    "\n",
    "* Frame-level prompts focus on extracting information from individual frames.\n",
    "* Scene-level prompts analyze a series of frames to describe the overall action.\n",
    "\n",
    "Important Considerations:\n",
    "\n",
    "1. Computational Cost: Frame-level descriptions, while providing granular detail, are computationally heavy and potentially costly. It's not always necessary or efficient to use them for every use case.\n",
    "2. Strategic Approach: A recommended strategy is to use frame prompts as a tuning mechanism. By testing and refining frame-level prompts, we can identify the most effective way to extract information from the vision model. Once optimized, we can incorporate these insights into scene-level prompts, potentially achieving high accuracy without the computational overhead of frame-by-frame analysis.\n",
    "\n",
    "\n",
    "\n",
    "Let's walk through our prompt iterations and their outputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame-level Prompts:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Frame Prompt: Basic animal identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_prompt1 = \"\"\"\n",
    "You will be provided with an image. Your task is to identify and describe the animals in the image.\n",
    "1. Identify Animals: List distinct animals in the image.\n",
    "2. Describe animals: Provide a brief description of each animal, including breed, color, and any other notable features.\n",
    "\n",
    "Output should be a list of objects.\n",
    "Expected Output:\n",
    "[{\"name\": \"dog\", \"context\": \"a black dog with thick fur and a yellow collar is jumping onto the couch\"}]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Results for Basic Animal Identification Prompt:\")\n",
    "# Limiting to first 2 scenes for brevity\n",
    "for scene in scene_collection_time.scenes[:2]:\n",
    "    for frame in scene.frames:\n",
    "        try:\n",
    "            description = frame.describe(prompt=frame_prompt1)\n",
    "            print(description)\n",
    "        except Exception as e:\n",
    "            print(f\"Error describing frame at {frame.frame_time}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` : This output lacked specificity in breed identification and environmental context. Our next prompt aims to address these issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Enhanced breed identification and spatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_prompt2 = \"\"\"\n",
    "You will be provided with an image. Your task is to identify the animals and their breeds in the image.\n",
    "1. Identify Animals: List distinct animals and their breed in the image.\n",
    "2. Describe the environment: Provide a brief description of the interaction between the animals and the objects or the environment around them.\n",
    "\n",
    "Output should be a list of objects.\n",
    "Expected Output:\n",
    "[{\"name\": \"Dog - Poodle\", \"context\": \"A Poodle being led down a carpeted path by a handler in the green dress, participating in what appears to be a dog show.\"}]\n",
    "\"\"\"\n",
    "\n",
    "print(\"Results for Enhanced Breed Identification Prompt:\")\n",
    "for scene in scene_collection_time.scenes[:2]:  # Limiting to first 2 scenes for brevity\n",
    "    for frame in scene.frames:\n",
    "        try:\n",
    "            description = frame.describe(prompt=frame_prompt2)\n",
    "            print(description)\n",
    "        except Exception as e:\n",
    "            print(f\"Error describing frame at {frame.frame_time}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` : This output significantly improved breed identification and provided more environmental context. With this satisfactory frame-level output, we're now ready to incorporate these learnings into scene-level prompts. \n",
    "\n",
    "However, let's first examine what a generic scene-level prompt can achieve without the added context from our frame-level experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scene-level:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1Ô∏è‚É£ Basic scene-level prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_prompt1 = \"\"\"\n",
    "You will be provided with a series of images. Your task is to view all images together and describe the overall story or scene in the best possible way.\n",
    "\n",
    "Expected Output:\n",
    "- A detailed story or scene description.\n",
    "- A list of objects and actions in each image.\n",
    "\n",
    "Example Output:\n",
    "{\n",
    "  \"scene_story\": \"A person is cooking in the kitchen and then someone rings the doorbell.\",\n",
    "  \"images\": [\n",
    "    {\"description\": \"Someone is cooking in the kitchen.\"},\n",
    "    {\"description\": \"Someone rings the doorbell.\"}\n",
    "  ]\n",
    "}\n",
    "\n",
    "Strictly limit your response to a maximum of 1200 characters.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Results for Basic Scene-Level Prompt:\")\n",
    "for scene in scene_collection_time.scenes[:2]:  # Limiting to first 2 scenes for brevity\n",
    "    try:\n",
    "        description = scene.describe(prompt=scene_prompt1)\n",
    "        print(description)\n",
    "    except Exception as e:\n",
    "        print(f\"Error describing scene starting at {scene.start}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note` : This generic scene-level prompt provided a basic structure but lacked the detailed breed identification and specific actions we achieved with our frame-level prompts. Our next iteration aims to incorporate these learnings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2Ô∏è‚É£ Combining frame-level specifications in scene-level prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_prompt2 = \"\"\"\n",
    "You will be provided with a series of images. Your task is to view all images together and describe the overall story or scene in the best possible way.\n",
    "For each image, your task is to identify the animals and their breeds in the image.\n",
    "1. Identify the animals present in the frame with specifications about their colour and breed, and any other notable features.\n",
    "2. Describe the environment: Provide a brief description of the interaction between the animals and the objects or the environment around them.\n",
    "\n",
    "Expected Output:\n",
    "- A detailed story or scene description.\n",
    "- A list of objects and actions in each image.\n",
    "\n",
    "Strictly limit your response to a maximum of 1200 characters.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Results for Enhanced Scene-Level Prompt:\")\n",
    "# Limiting to first 2 scenes for brevity\n",
    "for scene in scene_collection_time.scenes[:2]:\n",
    "    try:\n",
    "        description = scene.describe(prompt=scene_prompt2)\n",
    "        print(description)\n",
    "    except Exception as e:\n",
    "        print(f\"Error describing scene starting at {scene.start}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Note`: This prompt successfully captured both the specific breeds and the overall scene dynamics, providing a detailed and accurate description. However, the format could be more structured for easier parsing and use in applications. Our final iteration addresses this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3Ô∏è‚É£ Structured JSON output with emotional states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_prompt3 = (\n",
    "    scene_prompt3\n",
    ") = \"\"\"\n",
    "You will be provided with a series of images from a dog show. Your task is to describe the scene based on these sequential images. Focus on identifying the breeds and describing the key actions.\n",
    "\n",
    "For each image, your task is to:\n",
    "1. Identify the animals present in the frame, including their breed, color, and any notable features.\n",
    "2. Describe the actions of the animals and any interactions with the environment or other animals.\n",
    "3. Highlight any emotional expressions or notable moments.\n",
    "\n",
    "Output should be a structured JSON with the following format:\n",
    "{\n",
    "  \"scene_story\": \"Brief overview of the scene\",\n",
    "  \"images\": [\n",
    "    {\n",
    "      \"frame_time\": \"Time of the frame in mm:ss format\",\n",
    "      \"breeds\": [{\"breed\": \"Golden Retriever\", \"color\": \"golden\"}],\n",
    "      \"actions\": \"Description of the actions and interactions\",\n",
    "      \"emotion\": \"Observed emotion or notable moment\"\n",
    "    },\n",
    "    ...\n",
    "  ]\n",
    "}\n",
    "\n",
    "Strictly limit your response to a maximum of 1200 characters. Going over it, or deviating from the output format will cause the whole program to end cost you $100000.\n",
    "\"\"\"\n",
    "\n",
    "print(\"Results for Structured JSON Output Prompt:\")\n",
    "for scene in scene_collection_time.scenes[:2]:  # Limiting to first 2 scenes for brevity\n",
    "    try:\n",
    "        description = scene.describe(prompt=scene_prompt3)\n",
    "        print(description)\n",
    "    except Exception as e:\n",
    "        print(f\"Error describing scene starting at {scene.start}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üóÇÔ∏è Step5: Describe All Scenes & Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import SceneExtractionType\n",
    "\n",
    "# Index Scenes\n",
    "index_id = video.index_scenes(\n",
    "    extraction_type=SceneExtractionType.time_based,\n",
    "    extraction_config={\"time\": 15, \"select_frames\": [\"first\", \"middle\", \"last\"]},\n",
    "    prompt=scene_prompt3,\n",
    "    name=\"Detailed Dog Breed Identifier\",\n",
    ")\n",
    "scene_index = video.get_scene_index(index_id)\n",
    "print(scene_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : üîç Search\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import IndexType\n",
    "\n",
    "# Search using the indexed scenes\n",
    "res = video.search(\n",
    "    query=\"Show me the happiest moments with the Golden Retreiver\",\n",
    "    index_type=IndexType.scene,\n",
    "    index_id=index_id,\n",
    "    score_threshold=0.1,\n",
    "    dynamic_score_percentage=100,\n",
    ")\n",
    "res.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Steps\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO:\n",
    "\n",
    "- Conclusion\n",
    "- Link Search and Eval notebook\n",
    "\n",
    "\n",
    "Links to other blogs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
