{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pO9hnPg8w0G"
      },
      "source": [
        "# üóíÔ∏è Lecture and Meeting Videos into Concise Notes with VideoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATyOkfiU8w0I"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/main/examples/lecture_notes_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjUobHuF8w0J"
      },
      "source": [
        "Taking notes during lectures or meetings can be tough, especially when trying to capture both what's on the screen and what's being said. To make it easier, we've developed a tool with VideoDB that combines different modalitites into searchable notes. Now, you can quickly find and share key moments from any session, whether it's a specific slide or a crucial point in the discussion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7WJgHAV8w0K"
      },
      "source": [
        "## Setup\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66p4vrIE8w0K"
      },
      "source": [
        "### üì¶  Installing packages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awQdOTpe8w0K"
      },
      "source": [
        "Before diving in, make sure you have the necessary tools installed:\n",
        "\n",
        "* VideoDB: For video indexing and search.\n",
        "* OpenAI: To generate text summaries.\n",
        "* Markdown2: For converting markdown text into HTML.\n",
        "\n",
        "To get started, install these packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nFjIoItLmf51"
      },
      "outputs": [],
      "source": [
        "%pip install videodb openai markdown2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh0GJJOj8w0L"
      },
      "source": [
        "## üîë API keys\n",
        "Before proceeding, ensure access to [VideoDB](https://videodb.io), [OpenAI](https://openai.com) API key. If not, sign up for API access on the respective platforms.\n",
        "\n",
        "> Get your API key from [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required** ) üéâ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Ns6s0Bmf52"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bl5zD_v-8w0M"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6Ap1eTq8w0M"
      },
      "source": [
        "## üìã Step 1: Connect to VideoDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDjSZ9IVmf53"
      },
      "outputs": [],
      "source": [
        "from videodb import connect\n",
        "\n",
        "conn = connect()\n",
        "coll = conn.get_collection()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TT-XcOL28w0M"
      },
      "source": [
        "## üé¨ Step 2: Upload the Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTkWckI48w0M"
      },
      "source": [
        "Next, let's upload our sample video:\n",
        "\n",
        "You can use any public url, Youtube link or local file on your system.\n",
        "\n",
        "This approach works particularly well with:\n",
        "\n",
        "* University lectures that include PowerPoint slides and detailed discussions.\n",
        "* Conference presentations where speakers use slides to illustrate their points.\n",
        "* Team meetings that involve screen sharing and in-depth explanations.\n",
        "\n",
        "In this tutorial, we're using an [explainer video](https://www.youtube.com/watch?v=QJNwK2uJyGs) focusing on ‚ÄúIntroduction to Arrays‚Äù. This example is perfect for demonstrating how to capture and summarize key points from both visual aids and spoken explanations in an educational setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DIQTx7Cmf53"
      },
      "outputs": [],
      "source": [
        "# Upload a video by url\n",
        "video = coll.upload(url=\"https://www.youtube.com/watch?v=QJNwK2uJyGs\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebw4_QaA8w0M"
      },
      "source": [
        "## üì∏üó£Ô∏è Step 3: Index the Video on different Modalities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JycbxU2L8w0M"
      },
      "source": [
        "Now comes the exciting part - we're going to index our video in two ways:\n",
        "\n",
        "1. Indexing spoken content (what's being said in the video)\n",
        "2. Indexing visual content (what's being shown in the video)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_hHNzXY8w0M"
      },
      "source": [
        "#### üó£Ô∏è Indexing Spoken Content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvCOfD3nmf53",
        "outputId": "d885520b-a80c-435b-b06e-119df17386f5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:47<00:00,  2.12it/s]\n"
          ]
        }
      ],
      "source": [
        "# Index spoken content\n",
        "\n",
        "video.index_spoken_words()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4Oakrtt8w0N"
      },
      "source": [
        "#### üëÅÔ∏è Indexing Visual Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oLRm8B68w0N"
      },
      "source": [
        "To learn more about Scene Index, explore the following guides:\n",
        "\n",
        "* [Quickstart Guide guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb) provides a step-by-step introduction to Scene Index. It's ideal for getting started quickly and understanding the primary functions.\n",
        "* [Scene Extraction Options Guide](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb) delves deeper into the various options available for scene extraction within Scene Index. It covers advanced settings, customization features, and tips for optimizing scene extraction based on different needs and preferences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9aBiPG28w0N"
      },
      "source": [
        "**1.Finding the Right Configuration for Scene Extraction**:\n",
        "\n",
        "In this case, our ideal result (notes) would depend on being able to accurately identify and extract every slide in the video. We use shot-based extraction to accurately identify transitions between slides, and thus create a scene corresponding to every slide. This involves some testing‚Äîstarting with a higher threshold to see the initial results, then lowering it to ensure all important slides are captured. By fine-tuning these settings, we can ensure every slide is accurately represented as a distinct scene.\n",
        "\n",
        "Let‚Äôs dive in and see how it works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nme-mKe8w0N"
      },
      "outputs": [],
      "source": [
        "from videodb import SceneExtractionType\n",
        "from IPython.display import Image, display\n",
        "import requests\n",
        "\n",
        "# Helper function that will help us view the Scene Collection Images\n",
        "def display_scenes(scenes, images=True):\n",
        "    for scene in scenes:\n",
        "        print(f\"{scene.id} : {scene.start}-{scene.end}\")\n",
        "        if images:\n",
        "            for frame in scene.frames:\n",
        "                # display(Image(data=image_data))\n",
        "                im = Image(requests.get(frame.url, stream=True).content)\n",
        "                display(im)\n",
        "        print(\"----\")\n",
        "\n",
        "\n",
        "scene_collection_default = video.extract_scenes(\n",
        "    extraction_type=SceneExtractionType.shot_based,\n",
        "    extraction_config={\"threshold\": 25, \"frame_count\": 1}\n",
        ")\n",
        "display_scenes(scene_collection_default.scenes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rSzoF0b8w0N"
      },
      "source": [
        "In this example, we begin with a higher threshold, but this may cause some important slides to be overlooked. To make sure we capture every slide, we'll adjust the threshold to a lower setting and re-run the extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GakHZQhggiY6"
      },
      "outputs": [],
      "source": [
        "from videodb import SceneExtractionType\n",
        "\n",
        "# Extract scenes using shot-based extraction\n",
        "scene_collection = video.extract_scenes(\n",
        "    extraction_type=SceneExtractionType.shot_based,\n",
        "    extraction_config={\"threshold\": 10, \"frame_count\": 1}\n",
        ")\n",
        "display_scenes(scene_collection.scenes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyNJZypH8w0N"
      },
      "source": [
        "Now that we have found the right configuration for Scene Indexing, it's like we've found the perfect match‚Äîlet's commit to indexing those scenes ‚ú®!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46wCeoS98w0N"
      },
      "source": [
        "**2. Indexing the Scenes for Easy Search**\n",
        "\n",
        "You can use the default prompt or customize it to fit your needs. Whether you want general notes or something more specific, tailoring the prompt lets you get exactly the information you're looking for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eYk1tmbftSp"
      },
      "outputs": [],
      "source": [
        "index_id = video.index_scenes(\n",
        "    extraction_type=SceneExtractionType.shot_based,\n",
        "    extraction_config={\"threshold\": 10, \"frame_count\": 1},\n",
        "    prompt=\"Imagine you are student studying for an exam. You are watching a given video lecture slide. Take notes from this slide and also think through if you can gather more information around the topic mentioned in the slide\"\n",
        ")\n",
        "scene_index = video.get_scene_index(index_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrqM2Jbs8w0N"
      },
      "source": [
        "## üìã Step 4: Processing the Video and Summarize each scene"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmqSE1qp8w0N"
      },
      "source": [
        "In this step, we'll combine the scene descriptions (equivalent to slide descriptions) and transcripts from each scene's duration, then ask the LLM to generate a comprehensive note. This approach allows our notes to integrate information from multiple modalities, including spoken words and visual content."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTtVFDudyp49"
      },
      "source": [
        "![](https://raw.githubusercontent.com/video-db/videodb-cookbook-assets/main/images/guides/lecture_notes_1.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUHVlW5M8w0N"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "summarize_prompt = \"\"\"\n",
        "Scene Description: {scene_description}\n",
        "Scene Transcript: {scene_transcript}\n",
        "\n",
        "You are an AI tasked with creating concise summary notes for scenes from a video, integrating both visual and spoken content. For each scene, you will receive a description of the visual content (e.g., slides, images, on-screen text) and a transcript of the spoken content. Your goal is to synthesize this information into a single, coherent summary that captures the main points discussed by the narrator at the time of each specific slide.\n",
        "\n",
        "Task:\n",
        "Identify key points from the spoken content (transcript) and determine their relevance.\n",
        "Integrate these key points with the most relevant visual elements (e.g., slides, images) shown during the scene.\n",
        "Focus on providing a cohesive summary that reflects the main discussion and key concepts presented.\n",
        "Constraints:\n",
        "It is not necessary to describe each visual element separately.\n",
        "Only include details that contribute to a clear understanding of the main topic discussed.\n",
        "The goal is to create a concise overall summary that encapsulates the essence of the explanation given during the scene.\n",
        "The summary should not exceed 200 characters.\n",
        "Output Format:\n",
        "Provide the summary in Markdown format under the heading \"### Scene Summary.\" The summary should seamlessly blend the spoken content and visual context, presenting a unified description of the main ideas conveyed during the scene.\n",
        "Example:\n",
        "Visual Content Description:\n",
        "\n",
        "\"A slide titled 'Introduction to Quantum Computing' with a diagram illustrating qubits and superposition.\"\n",
        "Spoken Content Transcript:\n",
        "\n",
        "\"In this slide, we discuss the basic concepts of quantum computing, focusing on the principle of superposition. Superposition allows qubits to exist in multiple states simultaneously, unlike classical bits.\"\n",
        "Example Output:\n",
        "Scene Summary: Introduction to Quantum Computing\n",
        "This scene introduces the fundamental concepts of quantum computing, focusing on the principle of superposition. The slide features a diagram showing qubits in multiple states, contrasting with classical bits.\"\"\"\n",
        "\n",
        "\n",
        "def extract_transcript_for_scene(transcript, start_time, end_time):\n",
        "    return \" \".join(\n",
        "        [item[\"text\"] for item in transcript if start_time <= item[\"start\"] < end_time]\n",
        "    )\n",
        "\n",
        "def summarize_scene(scene_description, scene_transcript):\n",
        "    client = openai.Client()\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"You are a helpful assistant that tasked with creating concise summary notes for scenes from a video, integrating both visual and spoken content. For each scene, you will receive a description of the visual content and a transcript of the spoken content. Your goal is to synthesize this information into a coherent summary that highlights the main points discussed while presenting the scene.\",\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": summarize_prompt.format(\n",
        "                    scene_description=scene_description,\n",
        "                    scene_transcript=scene_transcript,\n",
        "                ),\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def process_video(transcript, scenes):\n",
        "    summarized_scenes = []\n",
        "\n",
        "    for scene in scenes:\n",
        "        scene_transcript = extract_transcript_for_scene(\n",
        "            transcript, scene[\"start\"], scene[\"end\"]\n",
        "        )\n",
        "        summary = summarize_scene(scene[\"description\"], scene_transcript)\n",
        "\n",
        "        summarized_scenes.append(\n",
        "            {\n",
        "                \"start\": scene[\"start\"],\n",
        "                \"end\": scene[\"end\"],\n",
        "                \"image_url\": scene[\"image_url\"],\n",
        "                \"summary\": summary,\n",
        "            }\n",
        "        )\n",
        "\n",
        "    return summarized_scenes\n",
        "\n",
        "scenes = [\n",
        "    {\n",
        "        \"start\": scene.start,\n",
        "        \"end\": scene.end,\n",
        "        \"image_url\": scene.frames[0].url,\n",
        "        \"description\": scene_index[\"description\"],\n",
        "    }\n",
        "    for scene_index, scene in zip(scene_index, scene_collection.scenes)\n",
        "]\n",
        "transcript = video.get_transcript()\n",
        "summary = process_video(transcript, scenes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEswHDyD8w0O"
      },
      "source": [
        "## Step 5 : Display Summaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAezfGRI8w0O"
      },
      "source": [
        "Here we display summaries of video scenes using HTML. The summaries include images, scene timing, and text descriptions formatted with Markdown. The `create_summary_html` function generates the HTML structure, styling each summary as a card with an image and formatted text. The `display_summaries` function renders the generated HTML in an interactive, scrollable format, making it easy to visualize and review multiple scenes at once.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pDwCR4Z88w0O"
      },
      "outputs": [],
      "source": [
        "\n",
        "from IPython.display import HTML, display\n",
        "import markdown2\n",
        "\n",
        "def create_summary_html(summaries):\n",
        "    html_content = \"\"\"\n",
        "    <style>\n",
        "        .summary-container {\n",
        "            display: flex;\n",
        "            overflow-x: auto;\n",
        "            padding: 20px 0;\n",
        "            scroll-snap-type: x mandatory;\n",
        "            background-color: #f0f0f0;\n",
        "        }\n",
        "        .summary-item {\n",
        "            flex: 0 0 auto;\n",
        "            width: 300px;\n",
        "            margin-right: 20px;\n",
        "            border: 1px solid #ddd;\n",
        "            padding: 15px;\n",
        "            scroll-snap-align: start;\n",
        "            background-color: white;\n",
        "            box-shadow: 0 2px 5px rgba(0,0,0,0.1);\n",
        "        }\n",
        "        .summary-item img {\n",
        "            width: 100%;\n",
        "            height: 180px;\n",
        "            object-fit: cover;\n",
        "        }\n",
        "        .summary-item h3 {\n",
        "            margin-top: 10px;\n",
        "            color: #333;\n",
        "        }\n",
        "        .markdown-body {\n",
        "            font-family: Arial, sans-serif;\n",
        "            line-height: 1.6;\n",
        "            color: #333;\n",
        "        }\n",
        "        .markdown-body p {\n",
        "            margin-bottom: 10px;\n",
        "        }\n",
        "    </style>\n",
        "    <div class=\"summary-container\">\n",
        "    \"\"\"\n",
        "\n",
        "    for i, summary in enumerate(summaries, 1):\n",
        "        markdown_summary = markdown2.markdown(summary['summary'])\n",
        "        html_content += f\"\"\"\n",
        "        <div class=\"summary-item\">\n",
        "            <img src=\"{summary['image_url']}\" alt=\"Scene {i}\" onerror=\"this.onerror=null;this.src='https://via.placeholder.com/300x180.png?text=Image+Not+Available';\">\n",
        "            <h3>Scene {i} ({summary['start']:.2f}s - {summary['end']:.2f}s)</h3>\n",
        "            <div class=\"markdown-body\">{markdown_summary}</div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "    html_content += \"</div>\"\n",
        "    return html_content\n",
        "\n",
        "def display_summaries(summaries):\n",
        "    html = create_summary_html(summaries)\n",
        "    display(HTML(html))\n",
        "\n",
        "display_summaries(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NLJMUTC0iwW"
      },
      "source": [
        "##Conclusion\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OArwI7Ny0yBC"
      },
      "source": [
        "In this tutorial, we've explored a sophisticated approach to extracting and identifying key points from lecture and meeting videos by combining spoken word indexing with shot-based scene indexing. By leveraging VideoDB's robust multimodal indexing capabilities, we've developed a powerful workflow that enables you to efficiently create detailed summaries or notes from your videos, capturing everything from crucial discussion points to key slides in a presentation.\n",
        "\n",
        "This approach is highly adaptable and can be applied to various scenarios where summarization is essential, such as:\n",
        "\n",
        "* Summarizing Board Meetings: Capture key decisions by linking spoken discussions with slides or whiteboard content.\n",
        "* Creating Training Notes: Merge screen recordings with spoken instructions to develop comprehensive step-by-step summaries.\n",
        "* Documenting Q&A Sessions: Summarize questions and answers by indexing on-screen content with verbal responses for easy reference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGjZynQ8004c"
      },
      "source": [
        "###Further Resources\n",
        "To learn more about Scene Index, explore the following guides:\n",
        "\n",
        "- [Quickstart Guide](https://github.com/video-db/videodb-cookbook/blob/main/quickstart/Scene%20Index%20QuickStart.ipynb)\n",
        "- [Scene Extraction Options](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/playground_scene_extraction.ipynb)\n",
        "- [Advanced Visual Search](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/advanced_visual_search.ipynb)\n",
        "- [Custom Annotation Pipelines](https://github.com/video-db/videodb-cookbook/blob/main/guides/scene-index/custom_annotations.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJrVRPD38w0O"
      },
      "source": [
        "### üë®‚Äçüë©‚Äçüëß‚Äçüë¶ Support & Community\n",
        "\n",
        "If you have any questions or feedback. Feel free to reach out to us üôåüèº\n",
        "\n",
        "* [Discord](https://colab.research.google.com/corgiredirector?site=https%3A%2F%2Fdiscord.gg%2Fpy9P639jGz)\n",
        "* [GitHub](https://github.com/video-db)\n",
        "* [Email](mailto:ashu@videodb.io)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
