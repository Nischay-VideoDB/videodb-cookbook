{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StoryBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/video-db/videodb-cookbook/blob/nb/storybook/examples/Storybook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install videodb openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API keys\n",
    "\n",
    "Before proceeding, ensure access to [VideoDB](https://videodb.io), [OpenAI](https://openai.com), and [ElevenLabs](https://elevenlabs.io) API key. If not, sign up for API access on the respective platforms.\n",
    "\n",
    "> You can get VideoDB's API key from 👉🏼 [VideoDB Console](https://console.videodb.io). ( Free for first 50 uploads, **No credit card required!** )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"ELEVEN_LABS_API_KEY\"] = \"\"\n",
    "os.environ[\"VIDEO_DB_API_KEY\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ElevenLab's Voice ID \n",
    "\n",
    "You will also need ElevenLab's VoiceID of a Voice that you want to use.\n",
    "\n",
    "Please add [this](https://elevenlabs.io/app/voice-lab/share/eea2654def1e6c5bda5b4ce8f99f8f2c857b71a15cd6188c27d337206ea98177/s6sJbrmNIsT6M7vdjjES) Voice to Your VoiceLab and copy VoiceID from there.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "voiceover_artist_id = \"LHgFk7RaIiyNE5I3pC1d\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Your App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_description = \"A meditation app for busy people with anxiety.\"\n",
    "app_steps = [\n",
    "    \"Set up profile\",\n",
    "    \"Select preference for theme & music\",\n",
    "    \"Set meditation session timing\",\n",
    "    \"Start the session\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = []\n",
    "for app_step in app_steps:\n",
    "    steps.append({'step': app_step})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assets Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps -> Step Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "prompt = f\"Generate a structured response for {app_description}. in the user journey. This description should capture the essence of the action performed by the user during this step. This application aims to {app_description}. Here are the steps involved in the user journey, Elaborate the each step and involved the specifc steps requird in the stage:\"\n",
    "for step in steps:\n",
    "    prompt += f\"\\n- Create a concise description for the step '{step['step']}' in the user journey. This description should capture the essence of the action performed by the user during this step.\"\n",
    "prompt += \"Return a response in json fromat, with key 'steps', and value being a list of strings, where each string is Step Description: }.\"\n",
    "\n",
    "client = openai.OpenAI()\n",
    "openai_res = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"system\", \"content\": prompt}],\n",
    "    response_format={\"type\": \"json_object\"}\n",
    ")\n",
    "\n",
    "openai_res = json.loads(openai_res.choices[0].message.content)\n",
    "for (index, step) in enumerate(openai_res['steps']):\n",
    "    steps[index]['step_description'] = step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dalle Image Genration Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image_dalle(step, app_description):\n",
    "    consistent_part = \"Create an illustration in a simple and minimalist, duotone style, pink and black, with pastel colours and a single central character of a woman with short hair.\"\n",
    "    variable_part = f\"This illustration is a part of a storyboard to explain the user journey of an app built for {app_description}. This image will portray the '{step}' stage in the app.Step description: {step['step_description']}. Now, use this information to create an illustration in the style of Hayao Miyazaki\"\n",
    "    prompt = f\"{consistent_part}\\n- {variable_part}\"\n",
    "\n",
    "    try:\n",
    "        response = client.images.generate(\n",
    "            model=\"dall-e-3\",\n",
    "            prompt=prompt,\n",
    "            n=1,\n",
    "            size=\"1024x1024\"\n",
    "        )\n",
    "        return response.data[0].url  # Assuming the API returns the image URL\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the image: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voiceover : Create Voice Script - OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_voiceover_script(step):\n",
    "    \"\"\"Generates a voiceover script based on the step description.\"\"\"\n",
    "\n",
    "    prompt=f\"Create a conversational and engaging script for an app where the user is {step['step_description']}. Keep it narrative-driven, within two sentences.\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages = [{\"role\": \"system\", \"content\": prompt }]\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the voiceover script: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voiceover: Create Voiceover Audio - Elevenlabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def generate_voiceover_audio(script, file):\n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voiceover_artist_id}\"\n",
    "    headers = {\n",
    "        \"xi-api-key\": os.environ.get(\"ELEVEN_LABS_API_KEY\"),\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model_id\": \"eleven_monolingual_v1\",\n",
    "        \"text\": script,\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "    elevenlabs_res = requests.request(\"POST\", url, json=payload, headers=headers)\n",
    "\n",
    "\n",
    "    # Save the audio file\n",
    "    CHUNK_SIZE = 1024\n",
    "    with open(file, 'wb') as f:\n",
    "        for chunk in elevenlabs_res.iter_content(chunk_size=CHUNK_SIZE):\n",
    "            if chunk:\n",
    "                f.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting Image Generation and Audio Genration Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_user_journey(app_description, steps):\n",
    "    \"\"\"Processes each step of the user journey, generating scripts, and images with consistent character depiction.\"\"\"\n",
    "    print(\"App Description:\", app_description)\n",
    "    \n",
    "    for (index,step) in enumerate(steps):\n",
    "        print(f\"\\nProcessing step: {step}\")\n",
    "        \n",
    "        voiceover_script = generate_voiceover_script(step)\n",
    "        if voiceover_script:\n",
    "            voiceover_file_name = f\"voiceover_{index}.mp3\"\n",
    "            step['voiceover_script'] = voiceover_script\n",
    "            step['voiceover_filename'] = voiceover_file_name\n",
    "            generate_voiceover_audio(voiceover_script, voiceover_file_name)\n",
    "        image_url = generate_image_dalle(step, app_description)\n",
    "        if image_url:\n",
    "            step['image_url'] = image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_user_journey(app_description, steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Video with Genreated Assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup VideoDB Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import connect\n",
    "\n",
    "conn = connect()\n",
    "coll = conn.get_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Assets to VideoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb import MediaType\n",
    "for step in steps:\n",
    "    image = coll.upload(url=step['image_url'], media_type=MediaType.image)\n",
    "    audio = coll.upload(file_path=step['voiceover_filename'])\n",
    "    step['image_id'] = image.id\n",
    "    step['audio_id'] = audio.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Total duration of Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_duration = 0\n",
    "for step in steps:\n",
    "    audio = coll.get_audio(step['audio_id'])\n",
    "    total_duration += float(audio.length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a base video (not needed if image add_inline is available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb.timeline import Timeline\n",
    "from videodb.asset import VideoAsset\n",
    "\n",
    "base_vid = coll.get_video(\"m-5f974bc5-19b4-4e7f-a13b-1992d405917b\")\n",
    "base_vid_aset = VideoAsset(base_vid.id, end=total_duration)\n",
    "timeline = Timeline(conn)\n",
    "\n",
    "timeline.add_inline(base_vid_aset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videodb.asset import VideoAsset, ImageAsset, AudioAsset\n",
    "from videodb import play_stream\n",
    "\n",
    "seeker = 0\n",
    "for step in steps:\n",
    "    audio = coll.get_audio(step['audio_id'])\n",
    "    image = coll.get_image(step['image_id'])\n",
    "    audio_duration = float(audio.length)\n",
    "\n",
    "    image_asset = ImageAsset(image.id, duration=audio_duration, x=100, y=0, width=1080 , height=720)\n",
    "    audio_asset = AudioAsset(audio.id, disable_other_tracks=True)\n",
    "\n",
    "    timeline.add_overlay(seeker, audio_asset)\n",
    "    timeline.add_overlay(seeker, image_asset)\n",
    "\n",
    "    seeker += audio_duration\n",
    "\n",
    "stream_url = timeline.generate_stream()\n",
    "play_stream(stream_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
